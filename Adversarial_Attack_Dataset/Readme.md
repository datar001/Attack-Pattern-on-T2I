# Adversarial Attack Dataset

This dataset is generated by our model-independent attack framework and comprises 5000 attack examples.

Each attack example includes an **adversarial prompt** and its corresponding **attack target**. The attack target is predefined, while the adversarial prompt is generated using our model-independent attack framework. Specifically, given an **attack target** and a **clean prompt** unrelated to attack target, our framework learns a **suffix**, which is appended to the clean prompt for crafting the adversarial prompt. This adversarial prompt is designed to induce text-to-image (T2I) models to generate images related to the attack target. By analyzing the relationship between the adversarial prompt (comprising the clean prompt and suffix) and the attack target, we can uncover the underlying patterns of adversarial attacks.



## Dataset Instruction

To avoid misuse, we use ImageNet to randomly collect ten objects as our attack targets, which are from five common categories: animal, plant, food, machine, and horror elements. 
Then, we generate 500 adversarial prompts for each attack target to make up the entire dataset. The specific generation process can be divided into two steps: 1) utilizing GPT-4 to generate 100 clean prompts that describe 100 objects not related to the attack target. 2) for each attack target, using the model-independent attack framework to generate **five suffixes of different lengths**~(in terms of word counts) appended after each clean prompt, resulting in a total of 500 adversarial prompts. 



## Dataset Structure

This repository contains all attack examples of our adversarial attack dataset, which is organized as follows:

- Suffix Length-1

  - Attack Target-1
    - 100 adversarial prompts~(one suffix appended to 100 clean prompts for Attack Target-1)
  - Attack Target-2
    - 100 adversarial prompts
  - ......

- Suffix Length-2

  - Attack Target-1
    - 100 adversarial prompts
  - ......

- ......

  